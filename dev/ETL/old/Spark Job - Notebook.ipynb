{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ac06d5-d2df-4a43-9626-83cb09b2397d",
   "metadata": {},
   "source": [
    "# To Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f4842d-5c71-4d36-bcf1-a4c30525fb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/10 11:54:49 WARN Utils: Your hostname, OutOne resolves to a loopback address: 127.0.1.1; using 192.168.1.230 instead (on interface wlp8s0)\n",
      "22/04/10 11:54:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/10 11:54:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"credentials/credentials-gs.json\"\n",
    "\n",
    "# https://kashif-sohail.medium.com/read-files-from-google-cloud-storage-bucket-using-local-pyspark-and-jupyter-notebooks-f8bd43f4b42e\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, TimestampType, ArrayType, LongType\n",
    "from pyspark.sql import DataFrameWriter\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName('spark-ETL-Tweets') \\\n",
    "  .getOrCreate()\n",
    "\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "project = spark._jsc.hadoopConfiguration().get('fs.gs.project.id')\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n",
    "\n",
    "BUCKET = 'cloud-computing-2122-bjr'\n",
    "BUCKET_LINK = 'gs://cloud-computing-2122-bjr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f925101-001f-4830-ae5f-728b930adaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector()\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "print(spark.sparkContext._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472e9c87-22db-4b3f-84c6-b5d38bf4475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'banana'\n",
    "READFILE_SCHEMA = StructType([StructField('csvfile', StringType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4be98b-aeca-46bf-89ef-79fdab23499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([[csv_file]], READFILE_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9aab09-6d35-4761-849b-be30a4fa5bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|csvfile|\n",
      "+-------+\n",
      "| banana|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb818e8-5e1d-48d0-9540-5d9ffd21df2b",
   "metadata": {},
   "source": [
    "## 0: Check which CSV's havent been read\n",
    "\n",
    "https://cloud.google.com/storage/docs/downloading-objects#storage-download-object-portion-python\n",
    "\n",
    "Need: \n",
    "* To Have such .txt in bucket\n",
    "* Pull .txt\n",
    "* Check against list of csv's in bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c084a6d0-93d1-419f-9c8e-b632c76f41a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from bucket cloud-computing-2122-bjr to local file control/read_files.txt.\n"
     ]
    }
   ],
   "source": [
    "# Read txt file of already processed CSV's\n",
    "def read_txt_blob(bucket_name, destination_file_name):\n",
    "    # https://stackoverflow.com/questions/48279061/gcs-read-a-text-file-from-google-cloud-storage-directly-into-python\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.get_blob(destination_file_name)\n",
    "    read_file = blob.download_as_text(encoding=\"utf-8\")\n",
    "    # Log instead of print\n",
    "    print(\n",
    "        \"Loaded from bucket {} to local file {}.\".format(\n",
    "            bucket_name, destination_file_name\n",
    "        )\n",
    "    )\n",
    "    return read_file.split('\\n')\n",
    "\n",
    "FILES_ALREADY_READ = read_txt_blob(BUCKET, 'control/read_files.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc66686-5a33-495d-b9a0-44954625289d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['220312.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILES_ALREADY_READ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa3435b-6293-4439-aa37-eee35023ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available data files\n",
    "def list_blobs_with_prefix(bucket_name, prefix):\n",
    "    # https://cloud.google.com/storage/docs/listing-objects#storage-list-objects-python\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
    "    list_of_csvs = []\n",
    "    \"\"\"\n",
    "    print(\"Blobs:\")\n",
    "    \"\"\"\n",
    "    for blob in blobs:\n",
    "        list_of_csvs.append(blob.name.split(\"/\")[-1])\n",
    "        # print(blob.name)\n",
    "    return list_of_csvs\n",
    "\n",
    "DATA_IN_BUCKET = list_blobs_with_prefix(BUCKET, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cda2a34-a633-4d18-a726-02102405c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_TO_PROCESS = [f'data/{FILE}' for FILE in DATA_IN_BUCKET if FILE not in FILES_ALREADY_READ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37879a32-4dc8-4a6f-a9ec-98978c7c07dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/220312-subset.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILES_TO_PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc68871-e363-4dcd-972c-591f3b09fcab",
   "metadata": {},
   "source": [
    "## 1: Read CSV into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5670877c-9bf4-410a-a1d9-505fed8bd9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## print(f\"{BUCKET_LINK}/{csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219f0e7-e009-4c92-90ba-81d3737d8b79",
   "metadata": {},
   "source": [
    "### 1.1: Define Schema of Global Data\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/   \n",
    "https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20be546d-544c-419d-9794-0872eac95800",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SCHEMA = StructType([ \\\n",
    "    StructField(\"RowID\",IntegerType(),False), \\\n",
    "    StructField(\"userId\",LongType(),False), \\\n",
    "    StructField(\"username\",StringType(),False), \\\n",
    "    StructField(\"acctdesc\",StringType(),True), \\\n",
    "    StructField(\"location\", StringType(), True), \\\n",
    "    StructField(\"n_following\", IntegerType(), True), \\\n",
    "    StructField(\"n_followers\", IntegerType(), True), \\\n",
    "    StructField(\"n_totaltweets\", IntegerType(), True), \\\n",
    "    StructField(\"usercreatedts\", TimestampType(), False), \\\n",
    "    StructField(\"tweetId\", LongType(), False), \\\n",
    "    StructField(\"tweetcreatedts\", TimestampType(), False), \\\n",
    "    StructField(\"retweetcount\", IntegerType(), False), \\\n",
    "    StructField(\"text\", StringType(), True), \\\n",
    "    StructField(\"hashtags\", StringType(), False), \\\n",
    "    StructField(\"language\", StringType(), True), \\\n",
    "    StructField(\"coordinates\", StringType(), True), \\\n",
    "    StructField(\"favorite_count\", IntegerType(), True), \\\n",
    "    StructField(\"extractedts\", TimestampType(), False) \\\n",
    "  ])\n",
    "\n",
    "# StructField(\"hashtags\", StructType([StructField('text', StringType(), False), \\\n",
    "# StructField('indices', ArrayType(IntegerType()), False)]),False), \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194300eb-fd60-4dc7-b211-13084a728ea1",
   "metadata": {},
   "source": [
    "### 1.2 Define Schema of Users DB\n",
    "\n",
    "Has to be in accordance with BigQuery Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60e237d6-d7f5-4c98-bed8-b45868d19a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS_SCHEMA = StructType([ \\\n",
    "    StructField(\"userId\",LongType(),False), \\\n",
    "    StructField(\"usercreatedts\",TimestampType(),False), \\\n",
    "    StructField(\"username\",StringType(),False), \\\n",
    "    StructField(\"acctdesc\",StringType(),True), \\\n",
    "    StructField(\"n_following\", IntegerType(), True), \\\n",
    "    StructField(\"n_followers\", IntegerType(), True), \\\n",
    "    StructField(\"n_totaltweets\", IntegerType(), True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b48048-a47a-4b7e-917b-e1fe7f0164ae",
   "metadata": {},
   "source": [
    "### 1.2 Define Schema of Tweets DB\n",
    "\n",
    "Has to be in accordance with BigQuery Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106c005-9f74-47c4-8ba8-58e6c310e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS_SCHEMA = StructType([ \\\n",
    "    StructField(\"tweetId\",LongType(),False), \\\n",
    "    StructField(\"tweetcreatedts\",TimestampType(),False), \\\n",
    "    StructField(\"userId\",LongType(),False), \\\n",
    "    StructField(\"location\",StringType(),True), \\\n",
    "    StructField(\"language\", StringType(), True), \\\n",
    "    StructField(\"retweetcount\", IntegerType(), True), \\\n",
    "    StructField(\"favorite_count\", IntegerType(), True), \\\n",
    "    StructField(\"tweet_text\", StringType(), True), \\\n",
    "    StructField(\"hashtags\", StringType(), False), \\\n",
    "    StructField(\"coordinates\", StringType(), True), \\\n",
    "    StructField(\"extractedts\", TimestampType(), False) \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149cec6-c03f-4cfd-a267-d1200ffff51e",
   "metadata": {},
   "source": [
    "### Connect to Users DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d14fb140-f83d-4389-ba6d-e81370e25f1d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o115.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: bigquery. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5337/580113607.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cadeira-nuvem-2122:bq_cloud_2122.db_users\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfUsers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigquery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cloud/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None,\n",
      "\u001b[0;32m~/miniconda3/envs/cloud/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cloud/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cloud/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o115.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: bigquery. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "USERS_TABLE_BIGQUERY = \"cadeira-nuvem-2122:bq_cloud_2122.db_users\"\n",
    "dfUsers = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\", table) \\\n",
    "  .load()\n",
    "\n",
    "## We only want the userId to compare\n",
    "dfUsers = dfUsers.select(\"userId\")\n",
    "\n",
    "dfUsers.write.mode(\"append\").saveAsTable(\"ALL_USERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b67699-173e-41c1-9f73-708689d37e64",
   "metadata": {},
   "source": [
    "### 1.3 Define Schema of Tweets DB\n",
    "\n",
    "Has to be in accordance with BigQuery Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cfc995-521f-4652-b867-4fef16b98d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8cc373c-179d-440e-bb1a-d2f65cac0cbd",
   "metadata": {},
   "source": [
    "### Iterate over CSV's\n",
    "\n",
    "* Control flow (Pull user data from BigQuery)\n",
    "* Update / Add Users\n",
    "* Add Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d500b-8eda-4110-b09e-06753afbcbaf",
   "metadata": {},
   "source": [
    "### Mock User data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b64ef6d-4085-4bc1-b3dd-b48a5f482e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>userId</th></tr>\n",
       "<tr><td>1068621782</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+\n",
       "|    userId|\n",
       "+----------+\n",
       "|1068621782|\n",
       "+----------+"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime;\n",
    "spark.sql('drop table if exists USER_DATA_TEST')\n",
    "columns = ['userId','usercreatedts', 'username', 'acctdesc', 'n_following', 'n_followers', 'n_totaltweets']\n",
    "data = [(1068621782, datetime.datetime.now(), 'BklynContractor', 'Nada', 1, 1, 2)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "spark.createDataFrame(rdd, schema = USERS_SCHEMA).toDF(*columns).select('userId').write.mode(\"append\").saveAsTable(\"USER_DATA_TEST\")\n",
    "spark.sql('SELECT * FROM USER_DATA_TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b80a1-8703-465e-80a2-3950c4cfe3de",
   "metadata": {},
   "source": [
    "### Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d9f81ff-fb66-4847-b458-c11f6a1f79fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/08 22:31:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: userid, username, acctdesc, following, followers, totaltweets, usercreatedts\n",
      " Schema: userId, username, acctdesc, n_following, n_followers, n_totaltweets, usercreatedts\n",
      "Expected: n_following but found: following\n",
      "CSV file: file:///home/portugapt/Documents/Cloud%202122/dev/ETL/input/220312-subset.csv\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/51751852/dataproc-reading-from-google-cloud-storage\n",
    "# https://stackoverflow.com/questions/61197811/can-i-read-csv-files-from-google-storage-using-spark-in-more-than-one-executor\n",
    "for csv_file in FILES_TO_PROCESS:\n",
    "    ## DOES NOT WORK\n",
    "    ##sc = SparkContext.getOrCreate()\n",
    "    ##rdd_csv = sc.wholeTextFiles(f\"{BUCKET_LINK}/{csv_file}\")\n",
    "    ##rdd_csv.collect()\n",
    "    ## WORKS - BUT DOES IT WORK FOR MULTIPLE FILES READ?\n",
    "    csv_file = (spark\n",
    "                .read\n",
    "                .format(\"csv\")\n",
    "                .option(\"wholeFile\", True)\n",
    "                .option(\"multiline\",True)\n",
    "                .option(\"header\", True)\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "                .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "                .schema(GLOBAL_SCHEMA)\n",
    "                .load(\"input/220312-subset.csv\"))\n",
    "    \n",
    "    csv_file.createOrReplaceTempView(\"DATA_FILE\")\n",
    "    \n",
    "    \n",
    "    #### START: USER DATA ####\n",
    "    ## Duplicated Users Data has the information for update. When it's implemented, should be the table giving the data to update users\n",
    "    spark.sql(\"\"\"SELECT A.* FROM DATA_FILE A INNER JOIN USER_DATA_TEST B ON A.userId == B.UserId\"\"\").write.mode(\"overwrite\").saveAsTable(\"DUPLICATED_USERS\")\n",
    "    \n",
    "    ### NEW_USER_DATA is the table that's thrown into bigquery\n",
    "    spark.sql(\"\"\"SELECT DISTINCT\n",
    "                    A.userId, \n",
    "                    A.usercreatedts, \n",
    "                    A.username, \n",
    "                    A.acctdesc, \n",
    "                    A.n_following, \n",
    "                    A.n_followers, \n",
    "                    A.n_totaltweets \n",
    "                FROM DATA_FILE A INNER JOIN DUPLICATED_USERS B ON A.userId <> B.UserId\"\"\").createOrReplaceTempView(\"NEW_USER_DATA\")\n",
    "    \n",
    "    ## This is the control table in the script, that if several CSV files are loaded, the chances of an user being loaded twice are nullified.\n",
    "    spark.sql(\"\"\"INSERT INTO TABLE USER_DATA_TEST SELECT userId FROM NEW_USER_DATA\"\"\")\n",
    "    \n",
    "    spark.sql('SELECT * FROM NEW_USER_DATA').write.mode('append') \\\n",
    "      .format('bigquery') \\\n",
    "      .option('table', USERS_TABLE_BIGQUERY)\n",
    "    \n",
    "    ### TODO ###\n",
    "    #UPDATE USER DATA THAT'S IN DUPLICATES\n",
    "    # CHANGE USER_DATA_TEST TO ALL_USERS\n",
    "    #### END: USER DATA ####\n",
    "    \n",
    "    #### START: Tweets Data ####\n",
    "    spark.sql(\"\"\"SELECT DISTINCT\n",
    "                A.tweetId, \n",
    "                A.tweetcreatedts, \n",
    "                A.userId, \n",
    "                A.location, \n",
    "                A.language, \n",
    "                A.retweetcount, \n",
    "                A.favouritecount,\n",
    "                A.tweettext,\n",
    "                A.hashtags,\n",
    "                A.coordinates,\n",
    "                A.extractedts\n",
    "                FROM DATA_FILE\"\"\").write.mode('append') \\\n",
    "                                  .format('bigquery') \\\n",
    "                                  .option('table', USERS_TABLE_BIGQUERY)\n",
    "    #### END: USER DATA ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10422ed6-5710-4b82-9b03-d82dc4bd857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/08 22:30:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: userid, username, acctdesc, following, followers, totaltweets, usercreatedts\n",
      " Schema: userId, username, acctdesc, n_following, n_followers, n_totaltweets, usercreatedts\n",
      "Expected: n_following but found: following\n",
      "CSV file: file:///home/portugapt/Documents/Cloud%202122/dev/ETL/input/220312-subset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"INSERT INTO TABLE USER_DATA_TEST SELECT userId FROM NEW_USER_DATA\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daca438-e0fe-4faf-be2e-b70645c64dce",
   "metadata": {},
   "source": [
    "### CSV file to User dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "112b2d78-58ba-4db4-826a-afdd4b20fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file.createOrReplaceTempView(\"DATA_FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28edbe16-232c-42ca-a25c-5acb91c0462e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>userId</th></tr>\n",
       "<tr><td>1068621782</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+\n",
       "|    userId|\n",
       "+----------+\n",
       "|1068621782|\n",
       "+----------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM DUPLICATED_USERS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04159a87-7fd7-4ae6-9d51-e6f8f8faff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT userId, usercreatedts, username, acctdesc, n_following, n_followers, n_totaltweets FROM DATA_FILE\").createOrReplaceTempView(\"NEW_USER_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666e837-98ff-4f1f-a202-ee150e10ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used\n",
    "# by the connector.\n",
    "spark.conf.set('temporaryGcsBucket', BUCKET)\n",
    "\n",
    "#\n",
    "\n",
    "words = spark.read.format('bigquery') \\\n",
    "  .option('table', 'bigquery-public-data:samples.shakespeare') \\\n",
    "  .load()\n",
    "words.createOrReplaceTempView('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6b710-8122-463f-81fe-a3110f8b0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweet = spark.read.option(\"header\",True).csv(\"./input/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5970c-6dad-48b0-b059-0a8ae312027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b1c83-3f08-4148-9494-961859073bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweet.select('userid', 'username').collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa66e93-cbc3-4afd-9e9c-66da28778244",
   "metadata": {},
   "source": [
    "# Write Read CSV's to control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536e916-5948-40a1-afff-bf113bacca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join FILES_TO_PROCESS and FILES_ALREADY_READ in PROCESSED_FILES\n",
    "PROCESSED_FILES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190194af-a3eb-419a-934f-e1e1f5533128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update txt file after data is processed\n",
    "# https://stackoverflow.com/questions/43682521/writing-data-to-google-cloud-storage-using-python\n",
    "def write_txt_to_bucket(bucket_name, destination_file_name, files_list):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_file_name) \n",
    "    ## Use bucket.get_blob('path/to/existing-blob-name.txt') to write to existing blobs\n",
    "    with blob.open(mode='w') as f:\n",
    "        f.write('\\n'.join(files_list))\n",
    "            \n",
    "write_txt_to_bucket('cloud-computing-2122-bjr', 'control/read_files.txt', PROCESSED_FILES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
